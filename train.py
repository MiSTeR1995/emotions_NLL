# train.py

import logging
import os
import datetime
import torch
import random
from torch.utils.data import DataLoader

from utils.logger_setup import setup_logger
from utils.config_loader import ConfigLoader
from data_loading.dataset_multimodal import DatasetMultiModal

def main():
    # –°–æ–∑–¥–∞—ë–º –ø–∞–ø–∫—É –¥–ª—è –ª–æ–≥–æ–≤ (–µ—Å–ª–∏ –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç)
    os.makedirs("logs", exist_ok=True)

    # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –∏–º—è —Ñ–∞–π–ª–∞ —Å –¥–∞—Ç–æ–π/–≤—Ä–µ–º–µ–Ω–µ–º
    datestr = datetime.datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
    log_file = os.path.join("logs", f"train_log_{datestr}.txt")

    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –ª–æ–≥–≥–µ—Ä: —Ü–≤–µ—Ç–Ω—ã–µ –ª–æ–≥–∏ –≤ –∫–æ–Ω—Å–æ–ª—å + –∑–∞–ø–∏—Å—å –≤ —Ñ–∞–π–ª
    setup_logger(logging.DEBUG, log_file=log_file)
    logging.info("üöÄ === –ó–∞–ø—É—Å–∫ on-the-fly —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫–∏ ===")

    # –ó–∞–≥—Ä—É–∂–∞–µ–º –∫–æ–Ω—Ñ–∏–≥
    config = ConfigLoader("config.toml")
    config.show_config()

    # –§–∏–∫—Å–∏—Ä—É–µ–º —Å–∏–¥, –µ—Å–ª–∏ —É–∫–∞–∑–∞–Ω –≤ –∫–æ–Ω—Ñ–∏–≥–µ
    if config.random_seed is not None:
        random.seed(config.random_seed)
        torch.manual_seed(config.random_seed)
        logging.info(f"üîí –§–∏–∫—Å–∏—Ä—É–µ–º random seed: {config.random_seed}")
    else:
        logging.info("üîì random seed –ù–ï —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω (None).")

    # –°–æ–∑–¥–∞—ë–º –¥–∞—Ç–∞—Å–µ—Ç
    dataset = DatasetMultiModal(
        csv_path=config.csv_path,
        wav_dir=config.wav_dir,
        emotion_columns=config.emotion_columns,
        split=config.split,
        sample_rate=config.sample_rate,
        wav_length=config.wav_length,
        whisper_model=config.whisper_model,
        max_text_tokens=config.max_text_tokens,
        text_column=config.text_column,
        use_whisper_for_nontrain_if_no_text=config.use_whisper_for_nontrain_if_no_text,
        whisper_device=config.whisper_device,
        subset_size=config.subset_size
    )

    # –°–æ–∑–¥–∞—ë–º DataLoader
    dataloader = DataLoader(
        dataset,
        batch_size=config.batch_size,
        shuffle=config.shuffle,
        num_workers=config.num_workers,
        collate_fn=custom_collate_fn
    )

    # –ü—Ä–∏–º–µ—Ä —Ü–∏–∫–ª–∞ –æ–±—É—á–µ–Ω–∏—è
    for epoch in range(2):
        logging.info(f"=== –≠–ø–æ—Ö–∞ {epoch} ===")
        for i, batch in enumerate(dataloader):
            if batch is None:
                continue

            audio = batch["audio"]
            labels = batch["label"]
            texts = batch["text"]

            logging.info(f"[Epoch={epoch} Batch={i}] audio_shape={audio.shape}, label_shape={labels.shape}")
            if texts:
                logging.info(f"–ü—Ä–∏–º–µ—Ä —Ç–µ–∫—Å—Ç–∞[0]: {texts[0]}")

    logging.info("‚úÖ –¢—Ä–µ–Ω–∏—Ä–æ–≤–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞.")

def custom_collate_fn(batch):
    batch = [x for x in batch if x is not None]
    if not batch:
        return None

    audios = [b["audio"] for b in batch]
    audio_tensor = torch.stack(audios)

    labels = [b["label"] for b in batch]
    label_tensor = torch.stack(labels)

    texts = [b["text"] for b in batch]

    return {
        "audio": audio_tensor,
        "label": label_tensor,
        "text": texts
    }

if __name__ == "__main__":
    main()
