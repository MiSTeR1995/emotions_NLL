import os
from data_loading.dataset_base import BaseDataset
import torch
import numpy as np

class DatasetMultiModal(BaseDataset):
    """–ú—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç –¥–ª—è –∞—É–¥–∏–æ, —Ç–µ–∫—Å—Ç–∞ –∏ —ç–º–æ—Ü–∏–π."""

    def __init__(self, csv_path, wav_dir, emotion_columns, split="train", modalities=None,
                 audio_processor=None, text_source="whisper", text_column="text"):
        super().__init__(csv_path, emotion_columns, text_column)

        if not os.path.exists(wav_dir):
            raise ValueError(f"–û—à–∏–±–∫–∞: –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è —Å –∞—É–¥–∏–æ {wav_dir} –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç!")

        self.wav_dir = wav_dir
        self.split = split
        self.modalities = modalities if modalities else ["audio"]
        self.audio_processor = audio_processor
        self.text_source = text_source
        self.text_column = text_column

        # üîπ –°–æ–∑–¥–∞—ë–º —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ {–ø—É—Ç—å –∫ –∞—É–¥–∏–æ: –∫–ª–∞—Å—Å —ç–º–æ—Ü–∏–∏} –¢–û–õ–¨–ö–û –î–õ–Ø train
        self.audio_class_map = {}
        if self.split == "train":
            print("üîÑ –°–æ–∑–¥–∞—ë–º `audio_class_map` –¥–ª—è train...")
            self.audio_class_map = {
                os.path.join(self.wav_dir, f"{row['video_name']}.wav"): self.get_emotion_label(row)
                for _, row in self.df.iterrows()
            }

    def __getitem__(self, idx):
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –æ–¥–∏–Ω —ç–ª–µ–º–µ–Ω—Ç (–∞—É–¥–∏–æ, —Ç–µ–∫—Å—Ç, —ç–º–æ—Ü–∏–∏)."""
        row = self.df.iloc[idx]

        # üîπ –§–æ—Ä–º–∏—Ä—É–µ–º –ø—É—Ç—å –∫ –∞—É–¥–∏–æ—Ñ–∞–π–ª—É
        audio_path = os.path.join(self.wav_dir, f"{row['video_name']}.wav")

        if not os.path.exists(audio_path):
            print(f"‚ö†Ô∏è –§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {audio_path}")
            return None

        # üîπ –ó–∞–≥—Ä—É–∂–∞–µ–º –∞—É–¥–∏–æ
        if self.audio_processor:
            waveform = self.audio_processor.load_audio(audio_path)
        else:
            waveform = None

        if waveform is None:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –∞—É–¥–∏–æ `{audio_path}`")
            return None

        # üîπ –ü–æ–ª—É—á–∞–µ–º —Ç–µ–∫—Å—Ç
        text = self.get_text(row, waveform)

        # üîπ –ó–∞–≥—Ä—É–∂–∞–µ–º —ç–º–æ—Ü–∏–∏
        emotion_vector = self.get_emotion_vector(row)

        return {
            "audio": waveform,   # –í–æ–∑–≤—Ä–∞—â–∞–µ–º *—Ç–µ–Ω–∑–æ—Ä* (waveform)
            "text": text,
            "label": emotion_vector
        }

    def get_text(self, row, waveform):
        """
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ç–µ–∫—Å—Ç:
        - –î–ª—è `train` - –≤—Å–µ–≥–¥–∞ Whisper.
        - –î–ª—è `dev` –∏ `test` - –ª–∏–±–æ Whisper, –ª–∏–±–æ –∏–∑ CSV, –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç text_source.
        """
        if self.split == "train" or self.text_source == "whisper":
            return self.audio_processor.text_processor.extract_text_from_waveform(waveform) if self.audio_processor else ""
        elif self.text_source == "csv":
            text = row[self.text_column] if self.text_column in row else ""
            return self.audio_processor.text_processor.trim_text(text)
        return ""

    def get_emotion_label(self, row):
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –Ω–∞–∑–≤–∞–Ω–∏–µ —ç–º–æ—Ü–∏–∏ (–Ω–∞–ø—Ä–∏–º–µ—Ä, 'happy', 'sad')."""
        emotion_values = row[self.emotion_columns].values.astype(float)
        max_index = np.argmax(emotion_values)  # –ò–Ω–¥–µ–∫—Å —Å–∞–º–æ–π —Å–∏–ª—å–Ω–æ–π —ç–º–æ—Ü–∏–∏
        return self.emotion_columns[max_index]
