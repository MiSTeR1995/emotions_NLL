import os
from data_loading.dataset_base import BaseDataset
from processing.audio_processing import AudioProcessor
import torch

class DatasetMultiModal(BaseDataset):
    """–ú—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç –¥–ª—è –∞—É–¥–∏–æ, —Ç–µ–∫—Å—Ç–∞ –∏ —ç–º–æ—Ü–∏–π."""

    def __init__(self, csv_path, wav_dir, emotion_columns, modalities=None, audio_processor=None, text_source="whisper", text_column="text"):
        """
        :param csv_path: –ü—É—Ç—å –∫ CSV —Å –º–µ—Ç–∫–∞–º–∏.
        :param wav_dir: –ü–∞–ø–∫–∞ —Å –∞—É–¥–∏–æ—Ñ–∞–π–ª–∞–º–∏.
        :param emotion_columns: –°–ø–∏—Å–æ–∫ –∫–æ–ª–æ–Ω–æ–∫ —Å —ç–º–æ—Ü–∏—è–º–∏ (–æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–π).
        :param modalities: –ö–∞–∫–∏–µ –º–æ–¥–∞–ª—å–Ω–æ—Å—Ç–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å (—Å–ø–∏—Å–æ–∫ ['audio']).
        :param audio_processor: –ü—Ä–µ–ø—Ä–æ—Ü–µ—Å—Å–æ—Ä –¥–ª—è –∞—É–¥–∏–æ –∏ —Ç–µ–∫—Å—Ç–∞.
        :param text_source: –ò—Å—Ç–æ—á–Ω–∏–∫ —Ç–µ–∫—Å—Ç–∞ ("whisper" –∏–ª–∏ "csv").
        :param text_column: –ö–æ–ª–æ–Ω–∫–∞ —Å —Ç–µ–∫—Å—Ç–æ–º –≤ CSV (–µ—Å–ª–∏ source = "csv").
        """
        super().__init__(csv_path, emotion_columns)

        if not os.path.exists(wav_dir):
            raise ValueError(f"–û—à–∏–±–∫–∞: –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è —Å –∞—É–¥–∏–æ {wav_dir} –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç!")

        self.wav_dir = wav_dir
        self.modalities = modalities if modalities else ["audio"]
        self.audio_processor = audio_processor
        self.text_source = text_source
        self.text_column = text_column

    def __getitem__(self, idx):
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –æ–¥–∏–Ω —ç–ª–µ–º–µ–Ω—Ç (–∞—É–¥–∏–æ, —Ç–µ–∫—Å—Ç, —ç–º–æ—Ü–∏–∏)."""
        row = self.df.iloc[idx]

        # üîπ –§–æ—Ä–º–∏—Ä—É–µ–º –ø—É—Ç—å –∫ –∞—É–¥–∏–æ—Ñ–∞–π–ª—É –Ω–∞ –æ—Å–Ω–æ–≤–µ `video_name`
        audio_path = os.path.join(self.wav_dir, f"{row['video_name']}.wav")

        if not os.path.exists(audio_path):
            print(f"‚ö†Ô∏è –§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {audio_path}")
            return None

        # üîπ –ó–∞–≥—Ä—É–∂–∞–µ–º –∞—É–¥–∏–æ
        audio = self.audio_processor.load_audio(audio_path) if self.audio_processor else None
        if audio is None:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –∞—É–¥–∏–æ `{audio_path}`")
            return None

        # üîπ –ü–æ–ª—É—á–∞–µ–º —Ç–µ–∫—Å—Ç
        text = self.get_text(audio_path, row)

        # üîπ –ó–∞–≥—Ä—É–∂–∞–µ–º —ç–º–æ—Ü–∏–∏
        emotion_vector = self.get_emotion_vector(row)

        return {
            "audio": audio,
            "text": text,
            "label": emotion_vector
        }

    def get_text(self, audio_path, row):
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ç–µ–∫—Å—Ç –∏–∑ CSV –∏–ª–∏ –∏–∑–≤–ª–µ–∫–∞–µ—Ç —Å Whisper."""
        if self.text_source == "csv":
            return row[self.text_column] if self.text_column in self.df.columns else ""
        elif self.text_source == "whisper":
            return self.audio_processor.transcribe_audio(audio_path) if self.audio_processor else ""
        return ""
