# data_loading/dataset_multimodal.py

import os
import random
import logging
import torch
import torchaudio
import whisper
import numpy as np
import pandas as pd
from torch.utils.data import Dataset

class DatasetMultiModal(Dataset):
    """
    –ú—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç –¥–ª—è –∞—É–¥–∏–æ, —Ç–µ–∫—Å—Ç–∞ –∏ —ç–º–æ—Ü–∏–π (–æ–Ω‚Äëthe‚Äëfly –≤–µ—Ä—Å–∏—è).

    –ü—Ä–∏ –∫–∞–∂–¥–æ–º –≤—ã–∑–æ–≤–µ __getitem__:
      - –ó–∞–≥—Ä—É–∂–∞–µ—Ç WAV –ø–æ video_name –∏–∑ CSV.
      - –î–ª—è –æ–±—É—á–∞—é—â–µ–π –≤—ã–±–æ—Ä–∫–∏ (split="train"):
            –ï—Å–ª–∏ –∞—É–¥–∏–æ –∫–æ—Ä–æ—á–µ target_samples, –≤—ã–ø–æ–ª–Ω—è–µ—Ç—Å—è —Ü–µ–ø–æ—á–∫–∞ —Å–∫–ª–µ–π–∫–∏:
            –≤—ã–±–∏—Ä–∞–µ—Ç—Å—è –æ–¥–∏–Ω –∏–ª–∏ –Ω–µ—Å–∫–æ–ª—å–∫–æ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤ —Ç–æ–≥–æ –∂–µ –∫–ª–∞—Å—Å–∞, –¥–∞–∂–µ –µ—Å–ª–∏ –æ–¥–∏–Ω –∫–∞–Ω–¥–∏–¥–∞—Ç –¥–ª–∏–Ω–Ω–µ–µ,
            –∏ –∏—Ç–æ–≥–æ–≤–æ–µ –∞—É–¥–∏–æ –∑–∞—Ç–µ–º –æ–±—Ä–µ–∑–∞–µ—Ç—Å—è –¥–æ —Ç–æ—á–Ω–æ–π –¥–ª–∏–Ω—ã.
      - –ï—Å–ª–∏ –∏—Ç–æ–≥–æ–≤–æ–µ –∞—É–¥–∏–æ –≤—Å—ë –µ—â—ë –º–µ–Ω—å—à–µ target_samples, –≤—ã–ø–æ–ª–Ω—è–µ—Ç—Å—è –ø–∞–¥–¥–∏–Ω–≥ –Ω—É–ª—è–º–∏.
      - –¢–µ–∫—Å—Ç –≤—ã–±–∏—Ä–∞–µ—Ç—Å—è —Ç–∞–∫:
            ‚Ä¢ –ï—Å–ª–∏ –∞—É–¥–∏–æ –±—ã–ª–æ merged (—Å–∫–ª–µ–µ–Ω–æ) ‚Äì –≤—ã–∑—ã–≤–∞–µ—Ç—Å—è Whisper –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –Ω–æ–≤–æ–≥–æ —Ç–µ–∫—Å—Ç–∞.
            ‚Ä¢ –ï—Å–ª–∏ merge –Ω–µ –ø—Ä–æ–∏—Å—Ö–æ–¥–∏–ª–æ –∏ CSV-—Ç–µ–∫—Å—Ç –Ω–µ –ø—É—Å—Ç ‚Äì –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è CSV-—Ç–µ–∫—Å—Ç.
            ‚Ä¢ –ï—Å–ª–∏ CSV-—Ç–µ–∫—Å—Ç –ø—É—Å—Ç–æ–π ‚Äì –¥–ª—è train (–∏–ª–∏, –ø—Ä–∏ —É—Å–ª–æ–≤–∏–∏, –¥–ª—è dev/test) –≤—ã–∑—ã–≤–∞–µ—Ç—Å—è Whisper.
      - –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ª–æ–≤–∞—Ä—å { "audio": waveform, "label": label_vector, "text": text_final }.
    """

    def __init__(
        self,
        csv_path,
        wav_dir,
        emotion_columns,
        split="train",
        sample_rate=16000,
        wav_length=2,
        whisper_model="tiny",
        max_text_tokens=15,  # –¥–∞–Ω–Ω—ã–π –ø–∞—Ä–∞–º–µ—Ç—Ä –±–æ–ª—å—à–µ –Ω–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è ‚Äì –ø–æ–ª–Ω—ã–π —Ç–µ–∫—Å—Ç –æ—Ç Whisper
        text_column="text",
        use_whisper_for_nontrain_if_no_text=True,
        whisper_device="cuda",
        subset_size=0
    ):
        """
        :param csv_path: –ü—É—Ç—å –∫ CSV-—Ñ–∞–π–ª—É (—Å –∫–æ–ª–æ–Ω–∫–∞–º–∏ video_name, emotion_columns, –≤–æ–∑–º–æ–∂–Ω–æ text).
        :param wav_dir: –ü–∞–ø–∫–∞ —Å –∞—É–¥–∏–æ—Ñ–∞–π–ª–∞–º–∏ (–∏–º—è —Ñ–∞–π–ª–∞: video_name.wav).
        :param emotion_columns: –°–ø–∏—Å–æ–∫ –∫–æ–ª–æ–Ω–æ–∫ —ç–º–æ—Ü–∏–π, –Ω–∞–ø—Ä–∏–º–µ—Ä ["neutral", "happy", "sad", ...].
        :param split: "train", "dev" –∏–ª–∏ "test".
        :param sample_rate: –¶–µ–ª–µ–≤–∞—è —á–∞—Å—Ç–æ—Ç–∞ –¥–∏—Å–∫—Ä–µ—Ç–∏–∑–∞—Ü–∏–∏ (–Ω–∞–ø—Ä–∏–º–µ—Ä, 16000).
        :param wav_length: –¶–µ–ª–µ–≤–∞—è –¥–ª–∏–Ω–∞ –∞—É–¥–∏–æ –≤ —Å–µ–∫—É–Ω–¥–∞—Ö.
        :param whisper_model: –ù–∞–∑–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ Whisper ("tiny", "base", "small", ...).
        :param max_text_tokens: (–ù–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è) ‚Äì –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –Ω–∞ —á–∏—Å–ª–æ —Ç–æ–∫–µ–Ω–æ–≤.
        :param text_column: –ù–∞–∑–≤–∞–Ω–∏–µ –∫–æ–ª–æ–Ω–∫–∏ —Å —Ç–µ–∫—Å—Ç–æ–º –≤ CSV.
        :param use_whisper_for_nontrain_if_no_text: –ï—Å–ª–∏ True, –¥–ª—è dev/test –ø—Ä–∏ –æ—Ç—Å—É—Ç—Å—Ç–≤–∏–∏ CSV-—Ç–µ–∫—Å—Ç–∞ –≤—ã–∑—ã–≤–∞–µ—Ç—Å—è Whisper.
        :param whisper_device: "cuda" –∏–ª–∏ "cpu" ‚Äì —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ –¥–ª—è –º–æ–¥–µ–ª–∏ Whisper.
        :param subset_size: –ï—Å–ª–∏ > 0, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —Ç–æ–ª—å–∫–æ –ø–µ—Ä–≤—ã–µ N –∑–∞–ø–∏—Å–µ–π –∏–∑ CSV (–¥–ª—è –æ—Ç–ª–∞–¥–∫–∏).
        """
        super().__init__()
        self.split = split
        self.sample_rate = sample_rate
        self.target_samples = int(wav_length * sample_rate)
        self.emotion_columns = emotion_columns
        self.whisper_model_name = whisper_model
        self.text_column = text_column
        self.use_whisper_for_nontrain_if_no_text = use_whisper_for_nontrain_if_no_text
        self.whisper_device = whisper_device

        # –ó–∞–≥—Ä—É–∂–∞–µ–º CSV
        if not os.path.exists(csv_path):
            raise ValueError(f"–û—à–∏–±–∫–∞: —Ñ–∞–π–ª CSV –Ω–µ –Ω–∞–π–¥–µ–Ω: {csv_path}")
        df = pd.read_csv(csv_path)
        if subset_size > 0:
            df = df.head(subset_size)
            logging.info(f"[DatasetMultiModal] –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ç–æ–ª—å–∫–æ –ø–µ—Ä–≤—ã–µ {len(df)} –∑–∞–ø–∏—Å–µ–π (subset_size={subset_size}).")

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –≤—Å–µ—Ö –∫–æ–ª–æ–Ω–æ–∫ —ç–º–æ—Ü–∏–π
        missing = [c for c in emotion_columns if c not in df.columns]
        if missing:
            raise ValueError(f"–í CSV –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ –∫–æ–ª–æ–Ω–∫–∏ —ç–º–æ—Ü–∏–π: {missing}")

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏–µ –ø–∞–ø–∫–∏ —Å –∞—É–¥–∏–æ
        if not os.path.exists(wav_dir):
            raise ValueError(f"–û—à–∏–±–∫–∞: –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è —Å –∞—É–¥–∏–æ {wav_dir} –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç!")
        self.wav_dir = wav_dir

        # –°–æ–±–∏—Ä–∞–µ–º —Å–ø–∏—Å–æ–∫ —Å—Ç—Ä–æ–∫: –¥–ª—è –∫–∞–∂–¥–æ–π –∑–∞–ø–∏—Å–∏ –ø–æ–ª—É—á–∞–µ–º –ø—É—Ç—å –∫ –∞—É–¥–∏–æ, label –∏ CSV-—Ç–µ–∫—Å—Ç (–µ—Å–ª–∏ –µ—Å—Ç—å)
        self.rows = []
        for i, rowi in df.iterrows():
            audio_path = os.path.join(wav_dir, f"{rowi['video_name']}.wav")
            if not os.path.exists(audio_path):
                continue
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –¥–æ–º–∏–Ω–∏—Ä—É—é—â—É—é —ç–º–æ—Ü–∏—é (–º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ)
            emotion_values = rowi[self.emotion_columns].values.astype(float)
            max_idx = np.argmax(emotion_values)
            emotion_label = self.emotion_columns[max_idx]

            # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–µ–∫—Å—Ç –∏–∑ CSV (–µ—Å–ª–∏ –µ—Å—Ç—å)
            csv_text = ""
            if self.text_column in rowi and isinstance(rowi[self.text_column], str):
                csv_text = rowi[self.text_column]

            self.rows.append({
                "audio_path": audio_path,
                "label": emotion_label,
                "csv_text": csv_text
            })

        # –°–æ–∑–¥–∞–µ–º –∫–∞—Ä—Ç—É –¥–ª—è –ø–æ–∏—Å–∫–∞ —Ñ–∞–π–ª–æ–≤ –¥–ª—è merge
        self.audio_class_map = {entry["audio_path"]: entry["label"] for entry in self.rows}

        logging.info("üìä –ê–Ω–∞–ª–∏–∑ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Ñ–∞–π–ª–æ–≤ –ø–æ —ç–º–æ—Ü–∏—è–º:")
        emotion_counts = {emotion: 0 for emotion in set(self.audio_class_map.values())}

        for path, emotion in self.audio_class_map.items():
            emotion_counts[emotion] += 1

        for emotion, count in emotion_counts.items():
            logging.info(f"üé≠ –≠–º–æ—Ü–∏—è '{emotion}': {count} —Ñ–∞–π–ª–æ–≤.")

        logging.info(f"[DatasetMultiModal] –°–ø–ª–∏—Ç={split}, –≤—Å–µ–≥–æ —Å—Ç—Ä–æ–∫: {len(self.rows)}")

        # –ó–∞–≥—Ä—É–∂–∞–µ–º Whisper-–º–æ–¥–µ–ª—å –æ–¥–∏–Ω —Ä–∞–∑ –Ω–∞ —É–∫–∞–∑–∞–Ω–Ω–æ–µ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ
        logging.info(f"–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è Whisper: –º–æ–¥–µ–ª—å={whisper_model}, —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ={whisper_device}")
        self.whisper_model = whisper.load_model(whisper_model, device=whisper_device)

    def __len__(self):
        return len(self.rows)

    def __getitem__(self, index):
        """
        –ó–∞–≥—Ä—É–∂–∞–µ—Ç –∏ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –æ–¥–∏–Ω —ç–ª–µ–º–µ–Ω—Ç –¥–∞—Ç–∞—Å–µ—Ç–∞ (–æ–Ω‚Äëthe‚Äëfly).
        """
        row = self.rows[index]
        audio_path = row["audio_path"]
        label_name = row["label"]
        csv_text = row["csv_text"]

        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º label –≤ one-hot –≤–µ–∫—Ç–æ—Ä
        label_vec = self.emotion_to_vector(label_name)

        # –®–∞–≥ 1. –ó–∞–≥—Ä—É–∂–∞–µ–º –∞—É–¥–∏–æ
        waveform, sr = self.load_audio(audio_path)
        if waveform is None:
            return None

        orig_len = waveform.shape[1]
        logging.debug(f"–ò—Å—Ö–æ–¥–Ω–∞—è –¥–ª–∏–Ω–∞ {os.path.basename(audio_path)}: {orig_len/sr:.2f} —Å–µ–∫")

        was_merged = False
        # –®–∞–≥ 2. –î–ª—è train, –µ—Å–ª–∏ –∞—É–¥–∏–æ –∫–æ—Ä–æ—á–µ target_samples, –ø—ã—Ç–∞–µ–º—Å—è –¥–æ–±–∞–≤–∏—Ç—å –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ —Ñ–∞–π–ª—ã (chain merge)
        if self.split == "train" and orig_len < self.target_samples:
            current_length = orig_len
            used_candidates = set()

            while current_length < self.target_samples:
                needed = self.target_samples - current_length
                candidate = self.get_suitable_audio(label_name, exclude_path=audio_path, min_needed=needed)
                if candidate is None or candidate in used_candidates:
                    break
                used_candidates.add(candidate)
                add_wf, add_sr = self.load_audio(candidate)
                if add_wf is None:
                    break
                logging.debug(f"–°–∫–ª–µ–π–∫–∞: –¥–æ–±–∞–≤–ª—è–µ–º {os.path.basename(candidate)} (–Ω–µ–æ–±—Ö–æ–¥–∏–º—ã—Ö —Å—ç–º–ø–ª–æ–≤: {needed})")
                waveform = torch.cat((waveform, add_wf), dim=1)
                current_length = waveform.shape[1]
            if current_length > orig_len:
                was_merged = True

        # –®–∞–≥ 3. –ï—Å–ª–∏ –∏—Ç–æ–≥–æ–≤–∞—è –¥–ª–∏–Ω–∞ –º–µ–Ω—å—à–µ target_samples, –≤—ã–ø–æ–ª–Ω—è–µ–º –ø–∞–¥–¥–∏–Ω–≥ –Ω—É–ª—è–º–∏
        curr_len = waveform.shape[1]
        if curr_len < self.target_samples:
            pad_size = self.target_samples - curr_len
            logging.debug(f"–ü–∞–¥–¥–∏–Ω–≥ {os.path.basename(audio_path)}: +{pad_size} —Å—ç–º–ø–ª–æ–≤")
            waveform = torch.nn.functional.pad(waveform, (0, pad_size))

        # –®–∞–≥ 4. –û–±—Ä–µ–∑–∞–µ–º –∏—Ç–æ–≥–æ–≤–æ–µ –∞—É–¥–∏–æ –¥–æ target_samples (–¥–∞–∂–µ –µ—Å–ª–∏ –ø–æ–ª—É—á–∏–ª–æ—Å—å –±–æ–ª—å—à–µ)
        waveform = waveform[:, :self.target_samples]
        logging.debug(f"–§–∏–Ω–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ {os.path.basename(audio_path)}: {waveform.shape[1]/sr:.2f} —Å–µ–∫; was_merged={was_merged}")

        # –®–∞–≥ 5. –ü–æ–ª—É—á–∞–µ–º —Ç–µ–∫—Å—Ç:
        # –ï—Å–ª–∏ –∞—É–¥–∏–æ –±—ã–ª–æ merged, –≤—ã–∑—ã–≤–∞–µ–º Whisper;
        # –ï—Å–ª–∏ –Ω–µ –±—ã–ª–æ –∏ CSV-—Ç–µ–∫—Å—Ç –Ω–µ–ø—É—Å—Ç–æ–π, –∏—Å–ø–æ–ª—å–∑—É–µ–º CSV-—Ç–µ–∫—Å—Ç;
        # –ò–Ω–∞—á–µ, –¥–ª—è train (–∏–ª–∏ –ø–æ —É—Å–ª–æ–≤–∏—é –¥–ª—è dev/test) –≤—ã–∑—ã–≤–∞–µ–º Whisper.
        if was_merged:
            logging.debug("–¢–µ–∫—Å—Ç: –∞—É–¥–∏–æ –±—ã–ª–æ merged ‚Äì –≤—ã–∑—ã–≤–∞–µ–º Whisper.")
            text_final = self.run_whisper(waveform)
        else:
            if csv_text.strip():
                logging.debug("–¢–µ–∫—Å—Ç: –∏—Å–ø–æ–ª—å–∑—É–µ–º CSV-—Ç–µ–∫—Å—Ç (–Ω–µ –ø—É—Å—Ç).")
                text_final = csv_text
            else:
                if self.split == "train" or self.use_whisper_for_nontrain_if_no_text:
                    logging.debug("–¢–µ–∫—Å—Ç: CSV –ø—É—Å—Ç–æ–π ‚Äì –≤—ã–∑—ã–≤–∞–µ–º Whisper.")
                    text_final = self.run_whisper(waveform)
                else:
                    logging.debug("–¢–µ–∫—Å—Ç: CSV –ø—É—Å—Ç–æ–π –∏ –Ω–µ –≤—ã–∑—ã–≤–∞–µ–º Whisper –¥–ª—è dev/test.")
                    text_final = ""

        return {
            "audio": waveform,
            "label": label_vec,
            "text": text_final
        }

    def load_audio(self, path):
        """
        –ó–∞–≥—Ä—É–∂–∞–µ—Ç –∞—É–¥–∏–æ –ø–æ —É–∫–∞–∑–∞–Ω–Ω–æ–º—É –ø—É—Ç–∏ –∏ —Ä–µ—Å—ç–º–ø–ª–∏—Ä—É–µ—Ç –µ–≥–æ –¥–æ self.sample_rate, –µ—Å–ª–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ.
        """
        if not os.path.exists(path):
            logging.warning(f"–§–∞–π–ª –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç: {path}")
            return None, None
        try:
            wf, sr = torchaudio.load(path)
            if sr != self.sample_rate:
                resampler = torchaudio.transforms.Resample(sr, self.sample_rate)
                wf = resampler(wf)
                sr = self.sample_rate
            return wf, sr
        except Exception as e:
            logging.error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ {path}: {e}")
            return None, None

    def get_suitable_audio(self, label_name, exclude_path, min_needed):
        """
        –ò—â–µ—Ç –∞—É–¥–∏–æ—Ñ–∞–π–ª —Å —Ç–æ–π –∂–µ —ç–º–æ—Ü–∏–µ–π, –¥–ª–∏–Ω–∞ –∫–æ—Ç–æ—Ä–æ–≥–æ >= min_needed (–≤ —Å—ç–º–ø–ª–∞—Ö).
        """
        candidates = [p for p, lbl in self.audio_class_map.items() if lbl == label_name and p != exclude_path]
        logging.debug(f"üîç –ù–∞–π–¥–µ–Ω–æ {len(candidates)} –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤ –¥–ª—è –∫–ª–∞—Å—Å–∞ '{label_name}'")

        valid = []
        for path in candidates:
            try:
                info = torchaudio.info(path)
                length = info.num_frames
                sr_ = info.sample_rate
                eq_len = int(length * (self.sample_rate / sr_)) if sr_ != self.sample_rate else length
                if eq_len >= min_needed:
                    valid.append((eq_len, path))
            except Exception as e:
                logging.warning(f"‚ö† –û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è {path}: {e}")

        logging.debug(f"‚úÖ –ü–æ–¥—Ö–æ–¥—è—â–∏—Ö —Ñ–∞–π–ª–æ–≤: {len(valid)} (–∏–∑ {len(candidates)})")

        if not valid:
            return None  # –ù–µ—Ç –ø–æ–¥—Ö–æ–¥—è—â–∏—Ö —Ñ–∞–π–ª–æ–≤

        # –í—ã–±–∏—Ä–∞–µ–º —Ñ–∞–π–ª —Å –º–∏–Ω–∏–º–∞–ª—å–Ω–æ –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ–π –¥–ª–∏–Ω–æ–π
        valid.sort(key=lambda x: x[0])
        return valid[0][1]


    def run_whisper(self, waveform):
        """
        –í—ã–∑—ã–≤–∞–µ—Ç Whisper –Ω–∞ –∞—É–¥–∏–æ—Å–∏–≥–Ω–∞–ª–µ –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –ø–æ–ª–Ω—ã–π —Ç–µ–∫—Å—Ç (–±–µ–∑ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è –ø–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤—É —Å–ª–æ–≤).
        """
        arr = waveform.squeeze().cpu().numpy()
        try:
            result = self.whisper_model.transcribe(arr, fp16=False)
            text = result["text"].strip()
            return text
        except Exception as e:
            logging.error(f"Whisper –æ—à–∏–±–∫–∞: {e}")
            return ""

    def emotion_to_vector(self, label_name):
        """
        –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç –Ω–∞–∑–≤–∞–Ω–∏–µ —ç–º–æ—Ü–∏–∏ –≤ one-hot –≤–µ–∫—Ç–æ—Ä (torch.tensor).
        """
        v = np.zeros(len(self.emotion_columns), dtype=np.float32)
        if label_name in self.emotion_columns:
            idx = self.emotion_columns.index(label_name)
            v[idx] = 1.0
        return torch.tensor(v, dtype=torch.float32)
