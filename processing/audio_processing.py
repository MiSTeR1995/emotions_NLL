import torchaudio
import torch
import os
import whisper
import unicodedata
import random
import numpy as np

class TextProcessor:
    """–ö–ª–∞—Å—Å –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ç–µ–∫—Å—Ç–∞ (–∏–∑–≤–ª–µ—á–µ–Ω–∏–µ, –æ–±—Ä–µ–∑–∫–∞ –ø–æ —Ç–æ–∫–µ–Ω–∞–º)."""

    def __init__(self, max_tokens=15, whisper_model="tiny"):
        """
        :param max_tokens: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤ (—Å–ª–æ–≤) –≤ —Ç–µ–∫—Å—Ç–µ.
        :param whisper_model: Whisper-–º–æ–¥–µ–ª—å –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Ç–µ–∫—Å—Ç–∞.
        """
        self.max_tokens = max_tokens
        self.whisper_model = whisper.load_model(whisper_model)

    def extract_text(self, audio_path):
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —Ç–µ–∫—Å—Ç –∏–∑ –∞—É–¥–∏–æ—Ñ–∞–π–ª–∞ —Å Whisper –∏ –æ–±—Ä–µ–∑–∞–µ—Ç –ø–æ —Ç–æ–∫–µ–Ω–∞–º."""
        if not os.path.exists(audio_path):
            print(f"‚ö†Ô∏è –§–∞–π–ª –¥–ª—è —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è –Ω–µ –Ω–∞–π–¥–µ–Ω: {audio_path}")
            return ""

        try:
            result = self.whisper_model.transcribe(audio_path)
            text = self.clean_text(result["text"])
            return self.trim_text(text)

        except Exception as e:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è —Ç–µ–∫—Å—Ç–∞ `{audio_path}`: {e}")
            return ""

    def extract_text_from_waveform(self, waveform):
        """
        –ò–∑–≤–ª–µ–∫–∞–µ—Ç —Ç–µ–∫—Å—Ç –∏–∑ –ø–µ—Ä–µ–¥–∞–Ω–Ω–æ–≥–æ –∞—É–¥–∏–æ—Å–∏–≥–Ω–∞–ª–∞ (waveform).
        –ï—Å–ª–∏ –ø—Ä–∏—à—ë–ª —Ç–µ–Ω–∑–æ—Ä PyTorch, –∫–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –µ–≥–æ –≤ NumPy (–∏ —É–¥–∞–ª—è–µ–º —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –∫–∞–Ω–∞–ª–∞, –µ—Å–ª–∏ –Ω—É–∂–Ω–æ).
        """
        # –ù–æ–≤–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞: –µ—Å–ª–∏ waveform ‚Äî —ç—Ç–æ PyTorch-—Ç–µ–Ω–∑–æ—Ä, –ø–µ—Ä–µ–≤–æ–¥–∏–º –≤ NumPy
        if isinstance(waveform, torch.Tensor):
            # –°–æ–∂–º—ë–º —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –∫–∞–Ω–∞–ª–∞ (B=1) –≤ (samples,) –∏ –ø–µ—Ä–µ–≤–µ–¥—ë–º –≤ NumPy
            waveform = waveform.squeeze(0).cpu().numpy()

        if not isinstance(waveform, np.ndarray):
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞: –æ–∂–∏–¥–∞–ª—Å—è np.ndarray –∏–ª–∏ torch.Tensor, –ø–æ–ª—É—á–µ–Ω–æ {type(waveform)}")
            return ""

        try:
            result = self.whisper_model.transcribe(waveform)
            text = self.clean_text(result["text"])
            return self.trim_text(text)

        except Exception as e:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è —Ç–µ–∫—Å—Ç–∞ –∏–∑ –∞—É–¥–∏–æ—Å–∏–≥–Ω–∞–ª–∞: {e}")
            return ""

    def trim_text(self, text):
        """–û–±—Ä–µ–∑–∞–µ—Ç —Ç–µ–∫—Å—Ç –ø–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤—É —Ç–æ–∫–µ–Ω–æ–≤ (—Å–ª–æ–≤)."""
        tokens = text.split()
        return " ".join(tokens[:self.max_tokens])

    @staticmethod
    def clean_text(text):
        """–û—á–∏—â–∞–µ—Ç —Ç–µ–∫—Å—Ç –æ—Ç –Ω–µ–ø–µ—á–∞—Ç–∞–µ–º—ã—Ö —Å–∏–º–≤–æ–ª–æ–≤ –∏ –Ω–æ—Ä–º–∞–ª–∏–∑—É–µ—Ç —é–Ω–∏–∫–æ–¥."""
        text = unicodedata.normalize("NFKC", text)
        text = text.encode("ascii", "ignore").decode("utf-8")  # –£–±–∏—Ä–∞–µ–º –Ω–µ-ASCII —Å–∏–º–≤–æ–ª—ã
        return text.strip()


class AudioProcessor:
    """–ö–ª–∞—Å—Å –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏, –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∞—É–¥–∏–æ –∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Ç–µ–∫—Å—Ç–∞."""

    def __init__(self, sample_rate=16000, wav_length=2, save_processed_audio=False,
                 output_dir="output_wavs", split="train", audio_class_map=None,
                 whisper_model="tiny", max_text_tokens=15):
        """
        :param sample_rate: –ß–∞—Å—Ç–æ—Ç–∞ –¥–∏—Å–∫—Ä–µ—Ç–∏–∑–∞—Ü–∏–∏ (–ì—Ü).
        :param wav_length: –î–ª–∏–Ω–∞ –∞—É–¥–∏–æ (–≤ —Å–µ–∫—É–Ω–¥–∞—Ö).
        :param save_processed_audio: –°–æ—Ö—Ä–∞–Ω—è—Ç—å –ª–∏ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–µ –∞—É–¥–∏–æ.
        :param output_dir: –ü–∞–ø–∫–∞ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤.
        :param split: –¢–∏–ø –≤—ã–±–æ—Ä–∫–∏ ("train", "dev", "test").
        :param audio_class_map: –°–ª–æ–≤–∞—Ä—å {–∞—É–¥–∏–æ_—Ñ–∞–π–ª: –∫–ª–∞—Å—Å_—ç–º–æ—Ü–∏–∏} –¥–ª—è —Å–∫–ª–µ–π–∫–∏.
        :param whisper_model: Whisper-–º–æ–¥–µ–ª—å –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ç–µ–∫—Å—Ç–∞.
        :param max_text_tokens: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ª–æ–≤ –≤ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–∏.
        """
        self.sample_rate = sample_rate
        self.wav_length = wav_length * sample_rate
        self.save_processed_audio = save_processed_audio
        self.output_dir = output_dir
        self.split = split
        self.audio_class_map = audio_class_map if audio_class_map else {}

        # –¢–µ–∫—Å—Ç–æ–≤—ã–π –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä (Whisper)
        self.text_processor = TextProcessor(
            max_tokens=max_text_tokens,
            whisper_model=whisper_model
        )

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ—Å—Ç—É–ø–Ω—ã–µ –∞—É–¥–∏–æ–±—ç–∫–µ–Ω–¥—ã
        available_backends = torchaudio.list_audio_backends()
        self.audio_backend = "sox_io" if "sox_io" in available_backends else \
                             "soundfile" if "soundfile" in available_backends else None

        if not self.audio_backend:
            print("‚ö†Ô∏è –í–Ω–∏–º–∞–Ω–∏–µ: –ù–µ—Ç –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –∞—É–¥–∏–æ–±—ç–∫–µ–Ω–¥–æ–≤. torchaudio.load() –º–æ–∂–µ—Ç –Ω–µ —Ä–∞–±–æ—Ç–∞—Ç—å.")

    def load_audio(self, path):
        """
        –ó–∞–≥—Ä—É–∂–∞–µ—Ç –∞—É–¥–∏–æ—Ñ–∞–π–ª, –∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –µ–≥–æ –¥–ª–∏–Ω—É, –ø—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏ —Å–∫–ª–µ–∏–≤–∞–µ—Ç (train)
        –∏–ª–∏ –ø–∞–¥–∏—Ç –Ω—É–ª—è–º–∏ (dev/test). –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ç–æ–ª—å–∫–æ waveform (—Ç–µ–Ω–∑–æ—Ä).
        """
        if not os.path.exists(path):
            print(f"‚ö†Ô∏è –§–∞–π–ª –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç: {path}")
            return None

        try:
            waveform, sample_rate = torchaudio.load(path, backend=self.audio_backend)
        except Exception as e:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ {path}: {e}")
            return None

        original_length = waveform.shape[1]
        print(f"üîπ –ò—Å—Ö–æ–¥–Ω–∞—è –¥–ª–∏–Ω–∞ –∞—É–¥–∏–æ `{os.path.basename(path)}`: {original_length / sample_rate:.2f} —Å–µ–∫")

        if self.split == "train":
            # üîÑ **–¢–æ–ª—å–∫–æ train: –°–∫–ª–µ–π–∫–∞ –∫–æ—Ä–æ—Ç–∫–∏—Ö –∞—É–¥–∏–æ—Ñ–∞–π–ª–æ–≤**
            if original_length < self.wav_length:
                print(
                    f"üîÑ –ê—É–¥–∏–æ `{os.path.basename(path)}` –∫–æ—Ä–æ—á–µ "
                    f"{self.wav_length / self.sample_rate:.2f} —Å–µ–∫, –∏—â–µ–º —Ñ–∞–π–ª –¥–ª—è —Å–∫–ª–µ–π–∫–∏..."
                )
                add_file = self.get_suitable_audio(path, self.wav_length - original_length)
                if add_file:
                    try:
                        add_waveform, _ = torchaudio.load(add_file, backend=self.audio_backend)
                        if add_waveform is not None:
                            waveform = torch.cat((waveform, add_waveform), dim=1)
                            print(f"üîó –°–∫–ª–µ–µ–Ω–æ —Å —Ñ–∞–π–ª–æ–º `{os.path.basename(add_file)}`")
                    except Exception as e:
                        print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ —Ñ–∞–π–ª–∞ –¥–ª—è —Å–∫–ª–µ–π–∫–∏ `{add_file}`: {e}")

        else:
            # üîπ **–î–ª—è `dev` –∏ `test`: –ü–∞–¥–¥–∏–Ω–≥ –Ω—É–ª—è–º–∏**
            if original_length < self.wav_length:
                pad_size = self.wav_length - original_length
                waveform = torch.nn.functional.pad(waveform, (0, pad_size))
                print(f"üîπ –ê—É–¥–∏–æ `{os.path.basename(path)}`: –¥–æ–±–∞–≤–ª–µ–Ω–æ {pad_size / sample_rate:.2f} —Å–µ–∫ –Ω—É–ª–µ–π.")

        # –û–±—Ä–µ–∑–∞–µ–º –¥–æ –Ω—É–∂–Ω–æ–π –¥–ª–∏–Ω—ã
        waveform = waveform[:, :self.wav_length]

        # ‚ùóÔ∏è –í–æ–∑–≤—Ä–∞—â–∞–µ–º —Ç–æ–ª—å–∫–æ —Ç–µ–Ω–∑–æ—Ä waveform (–±–µ–∑ sample_rate)
        return waveform

    def get_suitable_audio(self, original_path, min_needed_length):
        """–ò—â–µ—Ç —Ñ–∞–π–ª, –∫–æ—Ç–æ—Ä—ã–π –ø—Ä–∏ —Å–∫–ª–µ–π–∫–µ —Å –æ—Ä–∏–≥–∏–Ω–∞–ª–æ–º –¥–∞—Å—Ç –¥–ª–∏–Ω—É ‚â• `wav_length`."""
        emotion_label = self.audio_class_map.get(original_path)
        if not emotion_label:
            return None

        candidates = [
            path for path, label in self.audio_class_map.items()
            if label == emotion_label and path != original_path
        ]

        valid_files = []
        for path in candidates:
            if os.path.exists(path):
                waveform, _ = torchaudio.load(path)
                if waveform.shape[1] >= min_needed_length:
                    valid_files.append(path)

        return random.choice(valid_files) if valid_files else None
